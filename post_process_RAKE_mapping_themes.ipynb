{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alyazone/Quranic-Theme-Extraction-Visualization/blob/main/post_process_RAKE_mapping_themes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iQRbFiXcGXvW",
        "outputId": "4a82b8af-4b2b-455c-aeea-8a6e34ba3466"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'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'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DHwgALDFiaH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import re\n",
        "\n",
        "# Function to clean and flatten RAKE keywords\n",
        "def post_process_keywords(keywords):\n",
        "    \"\"\"\n",
        "    Cleans RAKE-extracted keywords by removing duplicates, normalizing,\n",
        "    and filtering unimportant words.\n",
        "    \"\"\"\n",
        "    if not isinstance(keywords, list):\n",
        "        try:\n",
        "            keywords = ast.literal_eval(keywords)  # Convert string to list\n",
        "        except Exception:\n",
        "            return [\"none\"]  # Fallback for malformed input\n",
        "\n",
        "    stopwords = {\"and\", \"or\", \"the\", \"of\", \"in\", \"to\", \"is\", \"for\", \"with\", \"by\", \"a\", \"an\"}\n",
        "    cleaned_keywords = list(set(  # Use `set` to remove duplicates\n",
        "        re.sub(r\"[^\\w\\s]\", \"\", kw).strip().lower()  # Remove special characters, trim, lowercase\n",
        "        for kw in keywords if len(kw) > 2 and kw.lower() not in stopwords\n",
        "    ))\n",
        "\n",
        "    return cleaned_keywords if cleaned_keywords else [\"none\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adJA9tDcGCgx",
        "outputId": "84a5ca1c-a4b7-4180-b299-6b460bc71adb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a semantic similarity model\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define theme-related terms\n",
        "theme_keywords = {\n",
        "    \"Forgiveness\": [\"forgiveness\", \"forgive\", \"forgiving\", \"forgave\", \"mercy\", \"repentance\", \"repent\", \"compassion\", \"pardon\", \"pardoned\"],\n",
        "    \"Truthfulness\": [\"truthfulness\", \"truthful\", \"truth\", \"honesty\", \"honest\", \"integrity\", \"sincerity\", \"veracity\"],\n",
        "    \"Patience\": [\"patience\", \"sabr\", \"endurance\", \"perseverance\", \"persevere\", \"resilience\", \"tolerance\", \"serenity\", \"steadfastness\"],\n",
        "    \"Gratitude\": [\"gratitude\", \"thankfulness\", \"appreciation\", \"gratefulness\", \"thankful\", \"grateful\", \"ungrateful\",\n",
        "                  \"recognition\", \"acknowledgement\", \"obligation\", \"obligations\", \"indebtedness\"],\n",
        "}\n",
        "\n",
        "# Pre-encode theme-related terms once\n",
        "theme_embeddings = {theme: model.encode(terms, convert_to_tensor=True) for theme, terms in theme_keywords.items()}\n",
        "\n",
        "def map_keywords_to_themes(keywords):\n",
        "    theme_embeddings = {theme: model.encode(terms, convert_to_tensor=True) for theme, terms in theme_keywords.items()}\n",
        "    theme_scores = {theme: 0 for theme in theme_keywords}\n",
        "\n",
        "    # Debugging\n",
        "    print(f\"Keywords being processed: {keywords}\")\n",
        "\n",
        "    for keyword in keywords:\n",
        "        keyword_embedding = model.encode(keyword, convert_to_tensor=True)\n",
        "        for theme, embeddings in theme_embeddings.items():\n",
        "            similarity = util.cos_sim(keyword_embedding, embeddings).max().item()\n",
        "            print(f\"Keyword: {keyword}, Theme: {theme}, Similarity: {similarity}\")\n",
        "            if similarity > 0.4:  # Lower threshold\n",
        "                theme_scores[theme] += 1\n",
        "\n",
        "    # Assign the theme with the highest score or fallback to 'Other'\n",
        "    return max(theme_scores, key=theme_scores.get) if max(theme_scores.values()) > 0 else \"Other\""
      ],
      "metadata": {
        "id": "CB79T889GCyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_refined_keywords(keywords, theme):\n",
        "    \"\"\"\n",
        "    Filters keywords to retain only those relevant to the given theme.\n",
        "    \"\"\"\n",
        "    theme_terms = theme_keywords[theme]\n",
        "    theme_embeddings = model.encode(theme_terms, convert_to_tensor=True)\n",
        "    filtered_keywords = []\n",
        "\n",
        "    for keyword in keywords:\n",
        "        keyword_embedding = model.encode(keyword, convert_to_tensor=True)\n",
        "        max_similarity = util.cos_sim(keyword_embedding, theme_embeddings).max().item()\n",
        "\n",
        "        # Include keywords with moderate relevance\n",
        "        if max_similarity > 0.4:  # Adjust threshold as needed\n",
        "            filtered_keywords.append(keyword)\n",
        "\n",
        "    return filtered_keywords if filtered_keywords else [\"none\"]"
      ],
      "metadata": {
        "id": "4c1iZMNISMdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to process a single file\n",
        "def process_file(file, theme):\n",
        "    df = pd.read_csv(file)\n",
        "\n",
        "    # Convert keywords from string to list format\n",
        "    df[\"Extracted Keywords\"] = df[\"Extracted Keywords\"].apply(\n",
        "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
        "    )\n",
        "\n",
        "    # Step 1: Post-process RAKE keywords\n",
        "    # df[\"Refined Keywords\"] = df[\"Extracted Keywords\"].apply(post_process_keywords)\n",
        "    print(f\"Before processing: Extracted Keywords (first 5 rows): {df['Extracted Keywords'].head()}\")\n",
        "    df[\"Refined Keywords\"] = df[\"Extracted Keywords\"].apply(post_process_keywords)\n",
        "    print(f\"After processing: Refined Keywords (first 5 rows): {df['Refined Keywords'].head()}\")\n",
        "\n",
        "\n",
        "    # Step 2: Map refined keywords to themes\n",
        "    # df[\"Mapped Theme\"] = df[\"Refined Keywords\"].apply(map_keywords_to_themes)\n",
        "    df[\"Mapped Theme\"] = df[\"Refined Keywords\"].apply(map_keywords_to_themes)\n",
        "    print(df[\"Mapped Theme\"].value_counts())\n",
        "\n",
        "\n",
        "    # Step 3: Assign the original theme (optional for comparison)\n",
        "    df[\"Original Theme\"] = theme\n",
        "\n",
        "    # Apply filtering after refinement\n",
        "    df[\"Filtered Refined Keywords\"] = df.apply(\n",
        "        lambda row: filter_refined_keywords(row[\"Refined Keywords\"], row[\"Mapped Theme\"]),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Debugging: Check rows with \"none\"\n",
        "    none_count = (df[\"Filtered Refined Keywords\"].apply(lambda x: x == [\"none\"])).sum()\n",
        "    print(f\"Rows with no relevant keywords (Filtered Refined Keywords == ['none']): {none_count}\")\n",
        "\n",
        "    other_rows = df[df[\"Mapped Theme\"] == \"Other\"]\n",
        "    print(f\"Rows mapped to 'Other': {len(other_rows)}\")\n",
        "    print(other_rows.head())\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "# List of files with corresponding themes\n",
        "files = [\n",
        "    (\"ahmedali-forgiveness-themes.csv\", \"Forgiveness\"),\n",
        "    (\"ahmedali-truthfulness-themes.csv\", \"Truthfulness\"),\n",
        "    (\"ahmedali-patience-themes.csv\", \"Patience\"),\n",
        "    (\"ahmedali-gratitude-themes.csv\", \"Gratitude\"),\n",
        "]\n",
        "\n",
        "processed_data = []\n",
        "\n",
        "# Process each file\n",
        "for file, theme in files:\n",
        "    try:\n",
        "        print(f\"Processing file: {file} for theme: {theme}\")\n",
        "        df = pd.read_csv(file)\n",
        "\n",
        "        # Step 1: Clean RAKE keywords\n",
        "        df[\"Cleaned Keywords\"] = df[\"Extracted Keywords\"].apply(post_process_keywords)\n",
        "\n",
        "        # Step 2: Refine Keywords and Filter by Theme\n",
        "        df[\"Refined Keywords\"] = df.apply(\n",
        "            lambda row: filter_refined_keywords(row[\"Cleaned Keywords\"], theme), axis=1\n",
        "        )\n",
        "\n",
        "        # Step 3: Map themes\n",
        "        df[\"Mapped Theme\"] = df[\"Refined Keywords\"].apply(map_keywords_to_themes)\n",
        "\n",
        "        processed_data.append(df)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file}: {e}\")\n",
        "\n",
        "# Merge processed data\n",
        "if processed_data:\n",
        "    final_df = pd.concat(processed_data, ignore_index=True)\n",
        "    final_df.to_csv(\"final-fine-tuning-dataset.csv\", index=False)\n",
        "    print(\"Final dataset saved as 'final-fine-tuning-dataset.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWo7CuKpGI1s",
        "outputId": "91a1f91c-2b77-44c3-9032-ef5f5c452e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before processing: Extracted Keywords (first 5 rows): 0            [name, merciful, ever, benevolent, allah]\n",
            "1                         [merciful, ever, beneficent]\n",
            "2    [lord sent commands, turned towards, kind, ind...\n",
            "3                      [pardoned, may, grateful, even]\n",
            "4    [softened towards, moses said, lord .\", lord, ...\n",
            "Name: Extracted Keywords, dtype: object\n",
            "After processing: Refined Keywords (first 5 rows): 0            [name, merciful, ever, benevolent, allah]\n",
            "1                         [merciful, ever, beneficent]\n",
            "2    [lord sent commands, turned towards, kind, ind...\n",
            "3                      [pardoned, may, grateful, even]\n",
            "4    [softened towards, moses said, lord, lord, tur...\n",
            "Name: Refined Keywords, dtype: object\n",
            "Mapped Theme\n",
            "Forgiveness     198\n",
            "Truthfulness     99\n",
            "Gratitude        40\n",
            "Patience          1\n",
            "Name: count, dtype: int64\n",
            "Rows with no relevant keywords (Filtered Refined Keywords == ['none']): 70\n",
            "Rows mapped to 'Other': 0\n",
            "Empty DataFrame\n",
            "Columns: [Chapter Number, Verse Number, Translation Verses, Extracted Keywords, Refined Keywords, Mapped Theme, Original Theme, Filtered Refined Keywords]\n",
            "Index: []\n",
            "Before processing: Extracted Keywords (first 5 rows): 0    [surah like, like, witness, votary, truthful, ...\n",
            "1    [gave adam knowledge, truthful .\", things, tel...\n",
            "2    [truth knowingly, confuse truth, falsehood, co...\n",
            "3    [gave moses, truth, remember, may, guided, fal...\n",
            "4    [truth ,\", good shape, brought us, blemish .\",...\n",
            "Name: Extracted Keywords, dtype: object\n",
            "After processing: Refined Keywords (first 5 rows): 0    [surah like, like, witness, votary, truthful, ...\n",
            "1    [gave adam knowledge, truthful, things, tell, ...\n",
            "2    [truth knowingly, confuse truth, falsehood, co...\n",
            "3    [gave moses, truth, remember, may, guided, fal...\n",
            "4    [truth, good shape, brought us, blemish, moses...\n",
            "Name: Refined Keywords, dtype: object\n",
            "Mapped Theme\n",
            "Truthfulness    125\n",
            "Forgiveness      77\n",
            "Gratitude        10\n",
            "Name: count, dtype: int64\n",
            "Rows with no relevant keywords (Filtered Refined Keywords == ['none']): 61\n",
            "Rows mapped to 'Other': 0\n",
            "Empty DataFrame\n",
            "Columns: [Chapter Number, Verse Number, Translation Verses, Extracted Keywords, Refined Keywords, Mapped Theme, Original Theme, Filtered Refined Keywords]\n",
            "Index: []\n",
            "Before processing: Extracted Keywords (first 5 rows): 0    [shall try, labour );, give tidings, wealth, s...\n",
            "1    [give us endurance, help us, truth .\", facing ...\n",
            "2    [hear many untoward things, straight path, hum...\n",
            "3    [may find success, fear god, suffering, streng...\n",
            "4    [yet god may, take perforce, open adultery, mu...\n",
            "Name: Extracted Keywords, dtype: object\n",
            "After processing: Refined Keywords (first 5 rows): 0    [shall try, labour, give tidings, wealth, sure...\n",
            "1    [give us endurance, help us, truth, facing gol...\n",
            "2    [hear many untoward things, straight path, hum...\n",
            "3    [may find success, fear god, suffering, streng...\n",
            "4    [yet god may, take perforce, open adultery, mu...\n",
            "Name: Refined Keywords, dtype: object\n",
            "Mapped Theme\n",
            "Forgiveness     10\n",
            "Truthfulness     7\n",
            "Gratitude        7\n",
            "Patience         6\n",
            "Name: count, dtype: int64\n",
            "Rows with no relevant keywords (Filtered Refined Keywords == ['none']): 10\n",
            "Rows mapped to 'Other': 0\n",
            "Empty DataFrame\n",
            "Columns: [Chapter Number, Verse Number, Translation Verses, Extracted Keywords, Refined Keywords, Mapped Theme, Original Theme, Filtered Refined Keywords]\n",
            "Index: []\n",
            "Before processing: Extracted Keywords (first 5 rows): 0    [devotional obligations, unknown, spend, given...\n",
            "1                      [pardoned, may, grateful, even]\n",
            "2    [word ), except, went back, others ),\", give z...\n",
            "3    [send ahead, devotional obligations, zakat, se...\n",
            "4    [give thanks, shall remember, remember, ungrat...\n",
            "Name: Extracted Keywords, dtype: object\n",
            "After processing: Refined Keywords (first 5 rows): 0    [devotional obligations, unknown, spend, given...\n",
            "1                      [pardoned, may, grateful, even]\n",
            "2    [word  except, went back, others, give zakat, ...\n",
            "3    [send ahead, devotional obligations, zakat, se...\n",
            "4    [give thanks, shall remember, remember, ungrat...\n",
            "Name: Refined Keywords, dtype: object\n",
            "Mapped Theme\n",
            "Gratitude       52\n",
            "Truthfulness    30\n",
            "Forgiveness     20\n",
            "Patience         1\n",
            "Name: count, dtype: int64\n",
            "Rows with no relevant keywords (Filtered Refined Keywords == ['none']): 34\n",
            "Rows mapped to 'Other': 0\n",
            "Empty DataFrame\n",
            "Columns: [Chapter Number, Verse Number, Translation Verses, Extracted Keywords, Refined Keywords, Mapped Theme, Original Theme, Filtered Refined Keywords]\n",
            "Index: []\n",
            "Processed File 1: Rows = 338\n",
            "Processed File 2: Rows = 212\n",
            "Processed File 3: Rows = 30\n",
            "Processed File 4: Rows = 103\n",
            "Total rows after merging all files: 683\n",
            "Rows with 'Other' before filtering: 0\n",
            "Rows after filtering 'Other': 683\n",
            "Final dataset saved as 'filtered-fine-tuning-dataset.csv'\n"
          ]
        }
      ]
    }
  ]
}